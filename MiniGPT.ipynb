{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sXTY4nwKbg50"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "tNzx4WFZc9Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "kcCluNuadS0Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()"
      ],
      "metadata": {
        "id": "O-nDq8QseG8Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary chars"
      ],
      "metadata": {
        "id": "wsE9LCEGfErC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_chars(text):\n",
        "  \"\"\"Removes all characters except alphanumeric characters (a-z), space, ?, and \\n from a text string.\n",
        "\n",
        "  Args:\n",
        "      text: The input text string.\n",
        "\n",
        "  Returns:\n",
        "      The text string with all special characters removed.\n",
        "  \"\"\"\n",
        "\n",
        "  allowed_chars = set(\"abcdefghijklmnopqrstuvwxyz \\n:,.?\")\n",
        "  filtered_text = ''.join([char for char in text if char in allowed_chars])\n",
        "  return filtered_text\n"
      ],
      "metadata": {
        "id": "2tkH4y9bfYsS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = remove_special_chars(text)\n"
      ],
      "metadata": {
        "id": "4c_azfP0f0HZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vocab from text and token mapping\n",
        "vocab_text = sorted(list(set(text)))\n",
        "\n",
        "ctoi = {k:i for i,k in enumerate(vocab_text)}\n",
        "ctoi\n",
        "itoc = {i:k for i,k in enumerate(vocab_text)}\n",
        "vocab_size = len(vocab_text)\n"
      ],
      "metadata": {
        "id": "lslMRyyIdkB5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf32jWFlewbe",
        "outputId": "57918bed-8c32-4268-9a81-ce94cec9eff6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize routines\n",
        "encode = lambda x: [ctoi[c] for c in x ]\n",
        "decode = lambda x: ''.join([itoc[c] for c in x])"
      ],
      "metadata": {
        "id": "D07019GbdtXB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text\n",
        "etext = encode(text)"
      ],
      "metadata": {
        "id": "lR2J-FSZg2aB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize text\n",
        "vtext = tf.constant(etext)"
      ],
      "metadata": {
        "id": "LesSWLIphAOc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we already have vocab_size. Lets get some more hyperparams\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "embed_size = 32\n",
        "num_heads = 4\n"
      ],
      "metadata": {
        "id": "eK-daYVzhXhy"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create Train and Val data\n",
        "n = int(0.9 * len(vtext))\n",
        "train_data = vtext[:n]\n",
        "val_data = vtext[n:]"
      ],
      "metadata": {
        "id": "u6fg12jkhlgs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  random_int = tf.random.uniform(shape=(batch_size, ), maxval=len(data) - block_size, minval=0, dtype=tf.int32)\n",
        "  x = tf.stack([data[i:i+block_size] for i in random_int])\n",
        "  y = tf.stack([data[i+1:i+block_size+1] for i in random_int])\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "RHYsI_gDhygP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Head implementing Attention.\n",
        "class Head(tf.keras.layers.Layer):\n",
        "  def __init__(self, head_size):\n",
        "    super(Head, self).__init__()\n",
        "    self.key = tf.keras.layers.Dense(head_size) #n_embed\n",
        "    self.query = tf.keras.layers.Dense(head_size)\n",
        "    self.value = tf.keras.layers.Dense(head_size)\n",
        "    self.tril = tf.ones((block_size, block_size))\n",
        "    self.tril = tf.linalg.band_part(self.tril, -1, 0)\n",
        "    self.dropout_layer = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    # Perform custom operations on the input\n",
        "    B,T,C = x.shape\n",
        "    k=self.key(x)\n",
        "    q=self.query(x)\n",
        "    k_t = tf.transpose(k, perm=(0,2,1))\n",
        "\n",
        "    wei = q @ k_t * C**-0.5\n",
        "\n",
        "    mask = tf.equal(self.tril, 0)\n",
        "    replace_value = -1e1000\n",
        "    wei = tf.where(mask, replace_value, wei)\n",
        "    wei = tf.nn.softmax(wei, axis=-1)\n",
        "    wei = self.dropout_layer(wei, training=True)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "zIVtewYfh5r0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHead(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super(MultiHead, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.head_size = head_size\n",
        "    #self.heads = tf.keras.layers.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.heads = []  # Use a list instead of ModuleList\n",
        "    for _ in range(num_heads):\n",
        "      self.heads.append(Head(head_size))\n",
        "\n",
        "    self.proj = tf.keras.layers.Dense(head_size*num_heads)\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    head_outputs = []\n",
        "    for head in self.heads:\n",
        "      out = head(x)  # Apply each head to the input\n",
        "      head_outputs.append(out)\n",
        "    outputs = tf.concat(head_outputs, axis=-1)\n",
        "    outputs = self.proj(outputs)\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "Gd0QDhCyiSUZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed Forward Layer which goes into attn block.\n",
        "class FFLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.n_embed = n_embed\n",
        "    self.net = tf.keras.layers.Dense(4*n_embed,activation=\"relu\") #n_embed\n",
        "    self.proj = tf.keras.layers.Dense(n_embed)\n",
        "    self.dropout_layer = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    out = self.net(x)\n",
        "    out = self.proj(out)\n",
        "    out = self.dropout_layer(out, training=True)\n",
        "    return out"
      ],
      "metadata": {
        "id": "n1gtl4_3i2a7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    self.head_size = n_embed//n_head\n",
        "    self.sa = MultiHead(n_head, self.head_size)\n",
        "    self.ffwd = FFLayer(n_embed)\n",
        "    self.ln1 = tf.keras.layers.LayerNormalization(axis=-1, input_shape=(n_embed,))\n",
        "    self.ln2 = tf.keras.layers.LayerNormalization(axis=-1, input_shape=(n_embed,))\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "ASQCi49MjHud"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer :\n",
        "  def __init__(self):\n",
        "\n",
        "    self.emb_layer = None\n",
        "    self.pos_table = None\n",
        "    self.emb_size = embed_size\n",
        "    self.pos_size = self.emb_size\n",
        "    self.num_multi_heads = num_heads\n",
        "\n",
        "    self.blockone = AttnBlock(self.emb_size, self.num_multi_heads)\n",
        "    self.blocktwo = AttnBlock(self.emb_size, self.num_multi_heads)\n",
        "    self.blockthree = AttnBlock(self.emb_size, self.num_multi_heads)\n",
        "    self.ln = tf.keras.layers.LayerNormalization(axis=-1, input_shape=(self.emb_size,))\n",
        "\n",
        "\n",
        "  def build_model(self):\n",
        "\n",
        "        self.emb_layer = tf.keras.layers.Embedding(vocab_size, self.emb_size, mask_zero=True, name=\"emb_layer\")\n",
        "        self.pos_table = tf.keras.layers.Embedding(block_size, self.pos_size)\n",
        "        pos_emb = self.pos_table(tf.range(0,8))\n",
        "\n",
        "        song_input = tf.keras.layers.Input(shape=(8,), dtype=tf.float32, name=\"Inputblock\")\n",
        "        song_embed = self.emb_layer(song_input)\n",
        "        cat_input = pos_emb+song_embed\n",
        "\n",
        "        x = self.blockone(cat_input)\n",
        "        x = self.blocktwo(x)\n",
        "        x = self.blockthree(x)\n",
        "        x = self.ln(x)\n",
        "\n",
        "        logits = tf.keras.layers.Dense(vocab_size, activation=None, use_bias=False)(x)\n",
        "\n",
        "        inputs = []\n",
        "        inputs.append((\"Inputblock\", song_input))\n",
        "\n",
        "        inputs = dict(inputs)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "WqLEfW2qjoWe"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "1VXNSw5XlAas"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets do 1 forward pass to make sure all is OK.\n",
        "GPTminiclass = Transformer()\n",
        "GPTmini = GPTminiclass.build_model()\n",
        "logits = GPTmini(xb)"
      ],
      "metadata": {
        "id": "4skql2-llGcC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Tape Gradient and cat cross entropy for backward pass.\n",
        "\n",
        "def loss_fn(yb, logits):\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  loss = loss_fn(yb, logits)\n",
        "  return loss\n",
        "\n",
        "def trainstep(xb, yb):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = GPTmini(xb, yb)\n",
        "    loss = loss_fn(yb, logits)\n",
        "  grads = tape.gradient(loss, GPTmini.trainable_variables)\n",
        "  watched_var = [var.name for var in tape.watched_variables()]\n",
        "  return grads, loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n"
      ],
      "metadata": {
        "id": "Ybg-ZLZYlxrl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets train the model\n",
        "for steps in range(500):\n",
        "  xb, yb = get_batch('train')\n",
        "  grads, currLoss = trainstep(xb, yb)\n",
        "  optimizer.apply_gradients(zip(grads, GPTmini.trainable_variables))"
      ],
      "metadata": {
        "id": "3h-EJ67omciP"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "currLoss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QI_V9QAmyYn",
        "outputId": "3c15febb-f227-4fda-8536-79a67c513ac8"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=2.073223>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference. Lets test the model.\n",
        "\n",
        "def generate(idx, max_new_tokens):\n",
        "\n",
        "      for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits = GPTmini(idx_cond)\n",
        "\n",
        "        logits = logits[:,-1,:]\n",
        "        idx_next = tf.random.categorical(logits, 1)\n",
        "        idx_next = tf.cast(idx_next, dtype=tf.int32)\n",
        "        idx = tf.concat([idx, idx_next], axis=1)\n",
        "      return idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eefa7_Skn5gU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testStr = \"that if \" # Seed to generate next sequence\n",
        "tokenTestStr = encode(testStr)\n",
        "vectorTestStr = tf.constant(tokenTestStr)\n",
        "idx = tf.reshape(vectorTestStr, (1,8))\n",
        "\n",
        "generatedTextTokens = generate(idx, 1000)\n",
        "numpyText = generatedTextTokens.numpy()\n",
        "numpyText = numpyText.reshape(-1)\n",
        "\n",
        "# Convert tokens back to text and print\n",
        "listRet = decode(list(numpyText))\n",
        "print(listRet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw8nBdD2pKkT",
        "outputId": "079f837e-2027-45ad-c9c8-bf30b84903dc"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that if horent caus beince, har gad, he couinome lmoht mere beistadeptach, wyoht,to the fethel.,,whe how, in scent: lead thouy tak the you sither thestald to foromegoorwithe., whather plive,gode.,,whhe hasrt tich and bara:,hee is dersebay in it broie. ,and thou key med,,a in tope manst thold be\n",
            "    ical\n",
            ".lewnfer o mmave hall sisicys:,there, hind twef delt hoveye or ie bince, hear with deent fird my the so f aledbarnd buttany the his. shis,he hinky as meth tome mosse shour monrea,, anevons ket,hen es,for somy ham chiviomath, yould theriphaked giove cell sat bpad,felloss .ditik, kenon likst hand,,offet ath oo, gey,hat his to he sirriifors, woee worth, wearild i habid dep, i and he wisse ghow,i ans, it is then reidsablerle\n",
            "        i poriie, with me suruesintrus off i n,to thou is nor quages, bus a groarnim an be me friemes dughids wine ird to gawar hank in i the thom me he ose, ond,.,what i:,you, flas baly lonfly tomt tocon moresins:,i what.,,padelabiny, let encerwarl iw ar pind intpeeot:sye que \n"
          ]
        }
      ]
    }
  ]
}